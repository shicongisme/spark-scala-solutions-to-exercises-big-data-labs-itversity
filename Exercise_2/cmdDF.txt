spark-shell --master yarn \
	--conf spark.ui.port=12345 \
	--num-executors 1 \
	--executor-cores 1 \
	--executor-memory 2G


import scala.io.Source

val ordersRaw = Source.fromFile("/data/retail_db/orders/part-00000").getLines.toList
val ordersRDD = sc.parallelize(ordersRaw)

val customersRaw = Source.fromFile("/data/retail_db/customers/part-00000").getLines.toList
val customersRDD = sc.parallelize(customersRaw)

val ordersDF = ordersRDD.
	map(o => o.split(",")(2).toInt).
	toDF("order_customer_id")

ordersDF.show

ordersDF.registerTempTable("orders_yashag66")

val customersDF = customersRDD.
	map(c => (c.split(",")(0).toInt, c.split(",")(1), c.split(",")(2))).
	toDF("customer_id", "customer_fname", "customer_lname")


customersDF.show

customersDF.registerTempTable("customers_yashag66")

sqlContext.sql("select customer_lname, customer_fname " +
	"from customers_yashag66 left outer join orders_yashag66 "+
	"on customer_id = order_customer_id "+
	"where order_customer_id is null "+
	"order by customer_lname, customer_fname").
	show


sqlContext.setConf("spark.sql.shuffle.partitions", "1")

sqlContext.sql("select customer_lname, customer_fname " +
	"from customers_yashag66 left outer join orders_yashag66 "+
	"on customer_id = order_customer_id "+
	"where order_customer_id is null "+
	"order by customer_lname, customer_fname").
	count


sqlContext.setConf("spark.sql.shuffle.partitions", "1")


hadoop fs -rm -R /user/yashag66/solutions/solutions02/inactive_customersDF


sqlContext.sql("select customer_lname, customer_fname " +
	"from customers_yashag66 left outer join orders_yashag66 "+
	"on customer_id = order_customer_id "+
	"where order_customer_id is null "+
	"order by customer_lname, customer_fname").
	rdd.
	map(rec => rec.mkString(", ")).
	saveAsTextFile("/user/yashag66/solutions/solutions02/inactive_customersDF")



