hadoop fs -copyFromLocal /data/nyse /user/yashag66/nyse

//to know the size of files in the directory
du -sh /data/nyse_parquet

spark-shell --master yarn \
	--conf spark.ui.port=12345 \
	--num-executors 4

val nyse = sc.textFile("/user/yashag66/nyse").
	map(stock => {
	val s = stock.split(",")
	(s(0), s(1), s(2).toFloat, s(3).toFloat, s(4).toFloat, s(5).toFloat, s(6).toInt)}).
	coalesce(4).
	toDF("stockticker", "transactiondate", "openprice", "highprice", "lowprice", "closeprice", "volume")

sqlContext.setConf("spark.sql.shuffle.partitions","4")

nyse.save("/user/yashag66/nyse_parquet","parquet")





